---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: IMAAX Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto Flex
    monofont: Liberation Mono
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

```{python}
#| echo: false


import os
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import matplotlib
import matplotlib.font_manager
import seaborn as sns

from requests import get
from urllib.parse import urlparse
```

```{python}
#| echo: false

from urllib.parse import urlparse
import os
from requests import get

def cache_data(src: str, dest: str) -> str:
    """
    Downloads and caches a file from the given URL if it does not already exist locally 
    or if the cached file is smaller than 250 bytes.

    Args:
        src (str): The source URL to download the file from.
        dest (str): The destination directory where the file should be stored.

    Returns:
        str: The full path to the cached or downloaded file.
    """
    url = urlparse(src)  # Parse the URL
    fn = os.path.split(url.path)[-1]  # Extract the filename
    dfn = os.path.join(dest, fn)  # Destination filename

    if not os.path.isfile(dfn) or os.path.getsize(dfn) < 250:
        path = os.path.split(dest)

        if len(path) >= 1 and path[0] != '':
            os.makedirs(os.path.join(*path), exist_ok=True)
            
        with open(dfn, "wb") as file:  # Write in binary
            response = get(src)
            file.write(response.content)

    return dfn
```

```{python}
#| echo: false


# load listing data

# Set download URL
ymd  = '20240614'
city = 'London'
host = 'https://orca.casa.ucl.ac.uk'
url  = f'{host}/~jreades/data/{ymd}-{city}-listings.csv.gz'

#download it locally if not exist

path = os.path.join('data','raw') 
fn   = url.split('/')[-1]         
#print(f"Writing to: {fn}")

df = pd.read_csv(cache_data(url, os.path.join('data','raw')))
#print(f"Data frame is {df.shape[0]:,} x {df.shape[1]}")
```

```{python}
#| echo: false
#| results: hide

# geo data download
ddir  = os.path.join('data','geo') # destination directory
spath = 'https://github.com/jreades/i2p/blob/master/data/src/' # source path

boros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )
water = gpd.read_file( cache_data(spath+'Water.gpkg?raw=true', ddir) )
green = gpd.read_file( cache_data(spath+'Greenspace.gpkg?raw=true', ddir) )
road =  gpd.read_file( cache_data(spath+'Roads.gpkg?raw=true', ddir) )
#print('Done.')
```

```{python}
#| echo: false

# airbnb listing
#df.info()

#choose cols needed for analysis


# Select relevant columns
cols = [
    'id', 'listing_url', 'last_scraped', 'name', 'description', 'host_id', 'host_name', 
    'host_since', 'host_location', 'host_about', 'host_acceptance_rate', 
    'host_is_superhost', 'host_neighbourhood', 'host_listings_count', 
    'host_total_listings_count', 'host_verifications', 'latitude', 'longitude',
    'property_type', 'room_type', 'accommodates', 'bathrooms', 'bathrooms_text', 
    'bedrooms', 'beds', 'amenities', 'price', 'minimum_nights', 'maximum_nights','minimum_minimum_nights',
    'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'availability_365', 'number_of_reviews', 'first_review', 'last_review', 
    'review_scores_rating', 'reviews_per_month'
]
```

```{python}
#| echo: false


# testing bottom and cols subset
testing = False


if testing:
    df = pd.read_csv(os.path.join(path,fn), 
                low_memory=False, nrows=10000, usecols=cols)
else:
    df = pd.read_csv(os.path.join(path,fn), 
                low_memory=False, usecols=cols)

#print(f"Data frame is {df.shape[0]:,} x {df.shape[1]}")
```

```{python}
#| echo: false


#get a summary table of Na in each column
na_counts = df.isnull().sum()
na_percentage = (df.isnull().mean() * 100).round(2)

na_summary = pd.DataFrame({'Missing Count': na_counts, 'Missing Percentage': na_percentage})

na_summary = na_summary[na_summary['Missing Count'] > 0].sort_values(by='Missing Percentage', ascending=False)

#print(na_summary)
```

```{python}
#| echo: false
#| results: hide
# store these ro#| ws to drop problem rows(with to many NA)
probs = df.isnull().sum(axis=1)
#print(type(probs))       
cutoff = 5
df.drop(probs[probs > cutoff].index, inplace=True)
#print(f"Have reduced data frame to: {df.shape[0]:,} rows and {df.shape[1]:,} columns")
```

```{python}
#| echo: false


# find cols should be bool but show object
bools = ['host_is_superhost']
df.sample(5, random_state=43)[bools]
# map 't' and 'f' to True and False
for b in bools:
    #print(f"Converting {b}")
    df[b] = df[b].replace({'f':False, 't':True}).astype('bool')
```

```{python}
#| echo: false


# find cols should be date but show object
dates = ['host_since']
#print(f"Currently {dates[0]} is of type '{df[dates[0]].dtype}'", "\n")
df.sample(5, random_state=43)[dates]

for d in dates:
    #print("Converting " + d)
    df[d] = pd.to_datetime(df[d])
#print(f"Now {dates[0]} is of type '{df[dates[0]].dtype}'", "\n")
```

```{python}
#| echo: false


# find cols should be cats but show object
cats = ['property_type','room_type']

#print(f"Currently {cats[1]} is of type '{df[cats[1]].dtype}'", "\n")
#df.sample(5, random_state=42)[cats]
```

```{python}
#| echo: false


# see unique value in cols and frequency
#print(df[cats[0]].value_counts())
#print(df[cats[1]].value_counts())

# convert dtype
for c in cats:
    #print(f"Converting {c}")
    df[c] = df[c].astype('category')
```

```{python}
#| echo: false


# convert object has numeric meaning 
money = ['price']
#df.sample(5, random_state=43)[money]

for m in money:
    #print(f"Converting {m}")
    df[m] = df[m].str.replace('$','', regex=False).str.replace(',','').astype('float')
```

```{python}
#| echo: false

# save data

path = os.path.join('data','clean')

if not os.path.exists(path):
    #print(f"Creating {path} under {os.getcwd()}")
    os.makedirs(path)
    
df.to_csv(os.path.join(path,fn), index=False)
#print("Done.")
```

```{python}
#| echo: false

# get geo version of df
# get listing data cleaned
df = pd.read_csv("data/clean/20240614-London-listings.csv.gz")

# get the gdf of listing data
gdf = gpd.GeoDataFrame(df,
      geometry=gpd.points_from_xy(df.longitude, df.latitude, crs='epsg:4326'))
      
# save the gdf version of listing data
gdf = gdf.to_crs(epsg=27700)

fn = "20240614-listings.gpkg"
file_path = os.path.join('data', 'geo', fn)

if not os.path.exists(file_path):
    try:
        gdf.to_file(file_path, driver='GPKG')
    except TypeError as e:
        pass  # Handle error silently or log it using a logging library if needed
```

```{python}
#| echo: false

# download   bio.bib  + reference style form github
ddir2 = os.getcwd()  # destination directory :qmd and this 2 file should be in the same folder
spath2 = 'https://github.com/aadarsh11421/IMAAX/blob/main/' # source path

cache_data(spath2+'bio.bib?raw=true', ddir2);
cache_data(spath2+'harvard-cite-them-right.csl?raw=true', ddir2);
```

## 1. Who collected the InsideAirbnb data?

::: {.duedate}

Answer

:::

An inline citation example: As discussed on @coxHowAirbnbsData, there are many...

A parenthetical citation example: There are many ways to research Airbnb [see, for example, @coxHowAirbnbsData,]... 

## 2. Why did they collect the InsideAirbnb data?

::: {.duedate}



:::





## 3. How was the InsideAirbnb data collected?  

::: {.duedate}



:::

## 4. How does the method of collection impact the completeness and/or accuracy of the InsideAirbnb data set's representation of the process it seeks to study, and what wider issues does this raise?

::: {.duedate}



:::

## 5. What ethical considerations does the use of the InsideAirbnb data raise? 

::: {.duedate}



:::

## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 

::: {.duedate}

### 6.1 Airbnb: Moving Beyond Home Sharing?
The proportion bar chart of London's Airbnb listings below reveals a potentially significant shift from its original home-sharing concept toward a more commercialized short-term rental market. Entire homes and apartments dominate the market, accounting for over 60% of all listings, while private rooms make up about 35%, leaving shared rooms and hotel rooms as minor segments at roughly 5% combined.

The booking patterns across all property types suggest a trend of professionalized approach to short-term rentals. Most properties show availability for booking 90-270 days into the future, indicating that hosts tend to plan their rental calendars far in advance. This long-term availability pattern is noteworthy - it suggests these properties might be primarily investment assets rather than primary residences, as ordinary homeowners would likely struggle to plan their personal space use so far ahead. However, it's also possible that some hosts can arrange their listings months in advance due to stable life and work schedules, and plan the use of spare rooms ahead of time. Further data is needed to validate this speculation.

The maximum stay duration suggests entire homes/apartments are targeting longer-term guests, while shared rooms and private rooms are targeting short-term guests. This could be a result of professional property management strategies, but it might also be partially due to the space limitations of shared and private rooms, which are less suitable for long-term rentals.

```{python}
#| echo: false
# define the room types in desired order
room_types = ['Entire home/apt', 'Private room', 'Shared room', 'Hotel room']

# create main figure
fig = plt.figure(figsize=(15, 12))
gs = fig.add_gridspec(4, 1, height_ratios=[1, 1, 1, 1])

# Define color scheme for room types
roomtype_color = {
    'Entire home/apt': '#d73027',  
    'Private room': '#7b3294',  
    'Shared room': '#ffd700',  
    'Hotel room': '#377eb8'   
}

# 1 Horizontal propotion bar chart
ax_top = fig.add_subplot(gs[0])
counts = df['room_type'].value_counts()
total = counts.sum()
proportions = pd.Series([counts.get(rt, 0) for rt in room_types], index=room_types) / total

left = 0
for rt in room_types:
    ax_top.barh(y=0, width=proportions[rt], left=left, color=roomtype_color[rt], label=rt)
    left += proportions[rt]

ax_top.set_title("Room Type Proportion and Stay Duration Distribution", fontsize=20, fontweight='bold')
ax_top.set_xlim(0, 1)
ax_top.set_xticks(np.arange(0, 1.1, 0.2))
ax_top.set_xticklabels([f'{int(x*100)}%' for x in np.arange(0, 1.1, 0.2)])
ax_top.set_yticks([])

# Add mean marker
mean_marker = plt.Line2D([], [], marker='^', color='none', markerfacecolor='green', markersize=10, label='Mean')
handles, labels = ax_top.get_legend_handles_labels()
ax_top.legend(handles + [mean_marker], labels + ['Mean'], title="Room Type", ncol=len(room_types) + 1, 
             loc='upper center', bbox_to_anchor=(0.5, -0.2))

# 2 Define metrics for boxplot 
metrics = [
    ('minimum_nights_avg_ntm', 'Minimum Nights in future Distribution', (0, 15)),
    ('maximum_nights_avg_ntm', 'Maximum Nights in future Distribution', (0, 200)),
    ('availability_365', 'Availability in future 365 Days Distribution', (0, 365))
]

# Create boxplots for each metric
for idx, (metric, title, xlim) in enumerate(metrics, 1):
    ax = fig.add_subplot(gs[idx])
    
    # Collect data for each room type
    metric_data = []
    for rt in room_types:
        # Filter data to remove outliers (0-365 days)
        data = df[(df['room_type'] == rt) & (df[metric] > 0) & (df[metric] < 365)][metric]
        metric_data.append(data)
    
    # Create boxplot
    bp = ax.boxplot(metric_data,
                    vert=False,
                    patch_artist=True,
                    boxprops=dict(linewidth=1.5),
                    whiskerprops=dict(linewidth=1.5),
                    medianprops=dict(color='black', linewidth=2.0),
                    capprops=dict(linewidth=1.5),
                    meanprops=dict(marker='^', markerfacecolor='green', markersize=10),
                    showmeans=True,
                    meanline=False,
                    showfliers=False)
    
    # Set box colors
    for box, rt in zip(bp['boxes'], room_types):
        box.set_facecolor(roomtype_color[rt])
        box.set_edgecolor('black')
    
    # Set chart properties
    ax.set_title(title, fontsize=14)
    ax.set_xlim(xlim)
    
    if metric == 'minimum_nights_avg_ntm':
        ax.set_xticks(np.arange(0, 16, 3))
    elif metric == 'maximum_nights_avg_ntm':
        ax.set_xticks(np.arange(0, 200, 30))
    else:
        ax.set_xticks(np.arange(0, 366, 90))
    
    ax.set_yticks(range(1, len(room_types) + 1))
    ax.set_yticklabels(room_types)
    ax.grid(True, axis='x', linestyle='-', alpha=0.3)
    
    ax.set_xlabel("Available Days" if metric == 'availability_365' else "Nights", fontsize=10)

plt.tight_layout()
plt.savefig('room_type_analysis.png', dpi=100, bbox_inches='tight', pad_inches=0.1)
plt.show()
```

```{python}
import plotly.express as px
import contextily as ctx

# Drop rows with missing price values
df = df.dropna(subset=['price'])

# Create the scatter mapbox plot
fig = px.scatter_mapbox(
    df,
    lon=df['longitude'],
    lat=df['latitude'],
    color=df['property_type'],  # Color markers by property type
    size=df['price'],  # Size markers by price
    hover_name=df['name'],
    hover_data={'price': True, 'availability_365': True, 'host_neighbourhood': True},
    zoom=10,
    height=900,
    title='Airbnb Listings in London'
)

# Update layout to use Stamen Toner style (black and white)
fig.update_layout(mapbox_style="carto-positron")

# Customize margins
fig.update_layout(margin={"r":0, "t":50, "l":0, "b":10})

# Show the plot
fig.show()
```

## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London? 



### 7.3 Focus on Camden: How Professional Hosts Influence Local Neighbourhoods?
Camden, with its mix of residential neighborhoods and thriving commercial hubs, offers an ideal case study to explore the community-level impact of professional Airbnb hosts.  The borough’s high concentration of short-term rentals (STRs), particularly entire properties [@doi:10.1177/0042098020970865], reflects its strong housing demand, cultural attractions, and economic activity.

Our analysis using reviews_per_month, a measure of property turnover, shows professional hosts dominate high-turnover areas like Camden Town and King’s Cross, which may reduce housing availability for long-term residents and increase pressure on the rental market [doi:10.1177/23998083211001836]. In contrast, non-professional hosts with single properties are more evenly distributed and play a more notable role in less commercial areas.

As shown in the POI count analysis, professional hosts are also clustered near key business amenities, particularly retail food establishments, and entertainment venues, indicating potential investment through visitor spending[@XU2021103670]. However, this economic benefit raises concerns about housing pressures, neighborhood stability, and cohesion.

```{python}
# 1load MSOA boundary to subset listing point
msoa_gpkg = gpd.read_file( cache_data(f'{host}/~jreades/data//MSOA-2011.gpkg', ddir) ).to_crs('epsg:27700')
listings = gpd.read_file('data/geo/20240614-listings.gpkg').to_crs('epsg:27700')

# choose study area
boro = 'Camden'
boro_gdf = msoa_gpkg[msoa_gpkg.LAD11NM==boro].copy()

# Do the spatial join
boro_listings = gpd.sjoin(listings, boro_gdf, predicate='within', rsuffix='_r')

# fill missing values with 0 to ignore
boro_listings['host_listings_count'] = boro_listings['host_listings_count'].fillna(0)

# 2 categorize hosts based on host_listings_count and group them by reviews and MSOA
def categorize_hosts(listings_count):
    if listings_count == 1:
        return 'Single-property host'
    elif 2 <= listings_count <= 5:
        return 'Mid-scale host'
    else:
        return 'Commercial host'

# apply the categorization to the DataFrame
boro_listings['host_category'] = boro_listings['host_listings_count'].apply(categorize_hosts)

# 1 separate professional and non-professional hosts data
# professional hosts = Commercial hosts + Mid-scale hosts
professional_hosts = boro_listings[
   (boro_listings['host_category'] == 'Commercial host') | 
   (boro_listings['host_category'] == 'Mid-scale host')
]

# non-professional hosts = Single-property hosts
nonprofessional_hosts = boro_listings[
   boro_listings['host_category'] == 'Single-property host'
]

# 2 calculate average monthly reviews for pro hosts in each MSOA
pro_reviews = professional_hosts.groupby('MSOA11NM')['reviews_per_month'].mean()
pro_reviews = pro_reviews.reset_index()
pro_reviews.columns = ['MSOA', 'pro_host_reviews']

# 3 calculate average monthly reviews for non-pro hosts in each MSOA
nonpro_reviews = nonprofessional_hosts.groupby('MSOA11NM')['reviews_per_month'].mean()
nonpro_reviews = nonpro_reviews.reset_index()
nonpro_reviews.columns = ['MSOA', 'nonpro_host_reviews']

# 4 Merge both datasets together
msoa_activity = pro_reviews.merge(
   nonpro_reviews, 
   on='MSOA',             
   how='outer'            
)

# 5 Merge statistics back to boro_gdf
boro_gdf = boro_gdf.merge(
    msoa_activity, 
    left_on='MSOA11NM',   
    right_on='MSOA', 
    how='left'
)
```

```{python}
import matplotlib.gridspec as gridspec

fig = plt.figure(figsize=(14, 10))  # Set the figure size
gs = gridspec.GridSpec(2, 2, figure=fig)  # 2 rows, 2 columns grid layout


# 1 Create bar chart comparing professional and non-professional hosts
ax3 = fig.add_subplot(gs[0, :])  
x = np.arange(len(msoa_activity['MSOA'])) 
width = 0.35  

# Plot bars for each host type
ax3.bar(x - width/2, msoa_activity['pro_host_reviews'], 
       width, label='Professional Hosts (Commercial + Mid-scale)')  
ax3.bar(x + width/2, msoa_activity['nonpro_host_reviews'], 
       width, label='Non-Professional Hosts (Single-property)')  

# Customize bar chart appearance
ax3.set_xlabel('MSOA')  # X-axis label
ax3.set_ylabel('Average Reviews Per Month')  
ax3.set_title('Average Monthly Reviews: Professional vs Non-Professional Hosts by MSOA in Camden', 
             fontweight='bold', fontsize=20)  
ax3.set_xticks(x)  # Set x-tick positions
ax3.set_xticklabels(msoa_activity['MSOA'], rotation=90)  
ax3.legend()  # Add legend

# 2 Create heatmaps showing geographical distribution

ax1 = fig.add_subplot(gs[1, 0])  
ax2 = fig.add_subplot(gs[1, 1], sharey=ax1)  #  share y-axis with left

# Calculate common color scale for both heatmaps
vmin = min(boro_gdf['pro_host_reviews'].min(), 
          boro_gdf['nonpro_host_reviews'].min())  
vmax = max(boro_gdf['pro_host_reviews'].max(), 
          boro_gdf['nonpro_host_reviews'].max())  

# Create heatmap for profosts
boro_gdf.plot(column='pro_host_reviews', 
             cmap='Reds',  
             legend=True, 
             vmin=vmin,  
             vmax=vmax,  
             ax=ax1)
ax1.set_title('Professional Hosts (Average Reviews Per Month)')

# Create heatmap for non-pro hosts
boro_gdf.plot(column='nonpro_host_reviews', 
             cmap='Reds', 
             legend=True, 
             vmin=vmin, 
             vmax=vmax,
             ax=ax2)
ax2.set_title('Non-Professional Hosts (Average Reviews Per Month)')


plt.tight_layout()  
plt.savefig('camden_airbnb_active.png', 
           dpi=100,  
           bbox_inches='tight',  
           pad_inches=0.1)  
plt.show()  
```

As shown in the POI count analysis, professional hosts are also clustered near key business amenities, particularly retail food establishments, and entertainment venues, indicating potential investment through visitor spending[@XU2021103670]. However, this economic benefit raises concerns about housing pressures, neighborhood stability, and cohesion.

```{python}
# down load poi we need from osm map
# this osm poi data from github is download from  the Geofabrik download server website(https://download.geofabrik.de/)
# then Navigate to: Europe > United Kingdom > England > Greater London
# Download greater-london-latest-free.shp.zip > gis_osm_pois_free_1.shp> transform it in Qgis to geopackge

# Set paths
ddir = os.path.join('data', 'geo')  # destination directory 
spath3 = 'https://github.com/Aprilmiaoyilee/IMAAX_miaoyi/blob/main/data/geo/'  # source path


# Read the complete shapefile into a GeoDataFrame

poi_gdf= gpd.read_file( cache_data(spath3+'gis_osm_pois.gpkg?raw=true', ddir) )





# poi list related according to reference
poi_list = [
    # Hotel-related
    "hotel", "hostel", "guesthouse", "motel", "caravan_site", "chalet",
    
    # Liquor-related
    "bar", "pub", "biergarten",
    
    # Retail Food-related
    "restaurant", "cafe", "fast_food", "food_court", "supermarket",
    "convenience", "greengrocer", "bakery", "butcher", "beverages",
    
    # Entertainment-related
    "cinema", "museum", "theatre", "arts_centre", "nightclub",
    "community_centre", "zoo", "monument", "memorial", "attraction",
    "park", "viewpoint"
]

filtered_poi = poi_gdf[poi_gdf["fclass"].isin(poi_list)].copy()
filtered_poi = filtered_poi.to_crs("EPSG:27700")


# fliterpoi to camden
poi_in_camden = gpd.sjoin(filtered_poi, boro_gdf, predicate="within", rsuffix="_r")
```

```{python}
#count poi in butter

# Step 1: Create 500-meter buffers
buffer_radius = 500
boro_listings["buffer"] = boro_listings.geometry.buffer(buffer_radius)

# Step 2: Perform spatial join to find POIs within buffers
buffer_gdf = boro_listings.set_geometry("buffer")
buffer_poi_intersect = gpd.sjoin(poi_in_camden, buffer_gdf, predicate="within", how="inner")

# Step 3: Count total POIs in buffers
poi_count = buffer_poi_intersect.groupby("index_right").size()
boro_listings["poi_count"] = boro_listings.index.map(poi_count).fillna(0)

# Step 4: Count POIs by type
poi_types = poi_in_camden["fclass"].unique()
for poi_type in poi_types:
    filtered_poi = buffer_poi_intersect[buffer_poi_intersect["fclass"] == poi_type]
    poi_type_count = filtered_poi.groupby("index_right").size()
    boro_listings[f"{poi_type}_count"] = boro_listings.index.map(poi_type_count).fillna(0)

# Step 5: Summarize POI counts
poi_summary = boro_listings.groupby("host_category")["poi_count"].mean()

poi_type_summary = boro_listings.groupby("host_category")[
    [f"{poi_type}_count" for poi_type in poi_types]
].mean()
```

```{python}
import seaborn as sns

# Define POI categories
categories = {
   "Hotel": ["hotel", "hostel", "guesthouse", "motel", "caravan_site", "chalet"],
   "Liquor": ["bar", "pub", "biergarten"],
   "Retail Food": ["restaurant", "cafe", "fast_food", "food_court", "supermarket",
                   "convenience", "greengrocer", "bakery", "butcher", "beverages"],
   "Entertainment": ["cinema", "museum", "theatre", "arts_centre", "nightclub",
                    "community_centre", "zoo", "monument", "memorial", "attraction",
                    "park", "viewpoint"]
}

# Calculate total POIs for each category
category_data = {}
for cat, poi_types in categories.items():
   valid_columns = [f"{poi}_count" for poi in poi_types if f"{poi}_count" in poi_type_summary.columns]
   category_data[cat] = poi_type_summary[valid_columns].sum(axis=1)

# Create DataFrame and plot heatmap
category_df = pd.DataFrame(category_data)

plt.figure(figsize=(12, 8))
sns.heatmap(category_df.T, annot=True, cmap="YlGnBu", cbar=True, fmt=".1f")
plt.title("POI Count within 500m of Listings: Host Categories vs. POI Category", 
        fontweight='bold', fontsize=20)
plt.ylabel("POI Categories")
plt.xlabel("Host Categories")

plt.savefig('POI count within 500m.png', dpi=100, bbox_inches='tight', pad_inches=0.1)
plt.show()
```

:::

## Sustainable Authorship Tools

Using the Terminal in Docker, you compile the Quarto report using `quarto render <group_submission_file>.qmd`.

Your QMD file should automatically download your BibTeX and CLS files and any other required files. If this is done right after library loading then the entire report should output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References
